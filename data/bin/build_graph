#!/usr/bin/env python

import click
import numpy as np
import datetime
import os
import json

from collections import OrderedDict
from textplot.utils import sort_dict
from textplot.text import Text
from textplot.matrix import Matrix
from humanist.graphs import Diachronic


@click.command()
@click.option('--freq_depth',   default=3000)
@click.option('--spike_depth',  default=1000)
@click.option('--skim_depth',   default=5)
@click.option('--bandwidth',    default=100000)
def build_graph(freq_depth, spike_depth, skim_depth, bandwidth):

    if not os.path.exists('build/kde'):
        os.makedirs('build/kde')

    print 'Tokenizing corpus...'
    t = Text.from_file('corpus.txt')
    m = Matrix(t)

    # Get the top X most frequent terms.
    frequent = t.most_frequent_terms(freq_depth)

    print 'Computing standard deviations...'
    spiky = OrderedDict()
    for term in frequent:
        spiky[term] = np.std(t.terms[term])

    # Sort by KDE max.
    spiky = sort_dict(spiky, False)
    terms = spiky.keys()[:spike_depth]

    print 'Indexing terms:'
    m.index(terms, bandwidth=bandwidth)

    g = Diachronic()

    print 'Generating graph:'
    g.build(m, skim_depth, bandwidth=bandwidth)
    g.write_gml('build/graph.gml')

    d1 = datetime.datetime(1987, 5, 12)
    d2 = datetime.datetime(2014, 9, 26)
    duration = (d2-d1).total_seconds()

    dates = []
    for i in xrange(100):
        diff = (float(i)/100) * duration
        date = d1 + datetime.timedelta(seconds=diff)
        dates.append(date.isoformat())

    # Write the KDEs.
    for term in terms:

        # Compute the KDE.
        kde = t.kde(term, bandwidth=bandwidth, samples=100)
        out = 'build/kde/'+t.unstem(term)+'.json'

        line = []
        vmax = max(kde)
        for i, v in enumerate(kde):
            line.append({'value': v/vmax, 'date': dates[i]})

        # Write the file.
        with open(out, 'w') as f:
            f.write(json.dumps(line))
            print out


if __name__ == '__main__':
    build_graph()
