# 27 years, 11 million words of the Humanist list

Back in september, I spent some time playing around with a little project called Textplot, which converts a document into a network of terms that are connected to one another depending on the extent to which they show up in the same parts of the document. Each word is converted into a probability density function across the text, which makes it possible to compute a statistical distance between that word and any other word in the document:

[fig]

From here, it's easy to build out a huge term matrix that keeps track of the similarities between all unique pairs of words. And, once that's in place, you can generate a little bespoke "topic" for any giving word just by popping out the 10-20 other words in the text that have the most similar distribution profiles. Eg, in war and peace, the 20 most similar words to "napoleon" are all about making war, giving orders, riding horses, etc:

[code]

These topics can then be treated as sets of weighted, directed edges in a network: napoleon connects to war with a weight of XXX, napoleon -> (XXX) order, napoleon -> (XXX) battle, etc. If you do this for _all_ of the words in the text, you end up with a big, comprehensive network of terms that teases out the underlying topic structure of the document. For example, war and peace becomes a big triangle - war to the left, peace on the right, and tolstoy's essays about historiography on the top:

[fig]

Now, in a lot of ways, the density functions look a lot like the time-series plots that crop up in lots of visualizations of historical feature counts over time - most notably, of course, the Google NGram viewer, but also projects like the New York Times' "Chronicle" tool, which does something similar for the NYT corpus going back to the mid-19th century. They're not exactly the same thing - the biggest difference is that the density functions are normalized so that they always trace out the same amount of area over the X axis, regardless of the how many times (or how few times) the word actually appears - this is what makes it possible to compare the distribution pattern of any two words in the document, even if one apears 1000 times and another appears 20 times. Whereas, the raw feature counts capture the absolute difference in frequency between different words. But, the gist is similar - both are capturing information about how something fluctuates over time.

With the visualizations of the novels, though, the "time" axis isn't really time at all, but rather what Matt Jockers calls the "novel time" of the text, the interval between the beginning and the end. This isn't totally divorced from regular time. The actual experience of reading a text takes place over the course of some number of minutes, hours, days, months, etc. And many (but certainly not all) texts progress chronologically from start to finish. But, at a computational level, "novel time" is really just the space between the first element in the array of tokenized words and the last - at core, it's _dimensional_, not temporal, information about how different features distribute across the one-dimensional axis of the document.

This got me thinking, though - what would happen if the x-axis actually was, in fact, "time" in the literal, historical sense of the idea? What if the text were actually a huge corpus of documents, daisy-chained together in chronological order into a sigle text, so that the "novel time" of the document actually corresponded to the "historical time" of the corpus? Eould textplot surface a kind of broad, diachronic shift in semantic focus over time? Usually, feature-count viewers like the google ngram viewer or the NYT chronical interface make it possible to compare the trend lines for a handful of specific queries. Eg, god vs. man, love vs. hate, etc:

[fig]

But, you have to supply the terms - and, implicitly, the questions. you can pluck out little threads of information based on preconceived notions about what you think is there, and what you think is important. But there's no way to step back and get a neutral, representative view of the entire corpus - a bird's eye view, an intuitive sense of how all the trend lines fit together. This reminds me of a really interesting conversation I had with Bess Sadler at Stanford last year about the question of how design better "browsing paradigms" for large digital collections. Conventional information retrieval systems like Solr or Elasticsearch are great when you already know what you're looking for - documents are treated as self-contained little atoms of information, and the assumption is that information seeking is always just a matter of finding the subset of documents that are most relevant to a given query. This is less useful, though, when the information need is exploratory, open-ended - when you're trying to grok the overall shape of the corpus/collection/data, not just do the equivalent of a big hash lookup to pop out a specific piece of data.

Now, this is probably related to the fact that I've spent altogether too much time in recent years working on mapping projects, and that everything looks like a nail when you've got a hammer, but - if you think about it, digital maps actually have a remarkably elegant solution to this type of problem. Think about the OpenStreetMap data set - it contains hundreds of millions of individual tiles. But, i can fire up a browser and, in just a couple seconds, zoom down to the one, individual tile at the highest zoom that shows my house, at the highest zoom level, in Menlo Park. It's incredibly easy to move back and forth between a high-level, synthetic view of the entire data set - the bay area, california, the united states, the world - and the individual pieces of information, which are automatically situated in the context of the whole.

Could this "spatial" browsing mechanic be applied to non-geospatial information? What if you could "zoom in" on a specific region of meaning in a text corpus in the same way that you can focus a map on San Francisco? There have been a couple of really interesting experiments to this effect recently - checkout randall olson's visualization of the [relationships between different subreddits](http://rhiever.github.io/redditviz), mario klingemann's [map of internet archive book subjects](http://incubator.quasimondo.com/internetarchive/InternetArchiveBookSubjectsMap.html), and matt miller's dizzyingly cool [force-directed atlas of NYLP subject headings](http://www.nypl.org/blog/2014/07/31/networked-catalog). I was curious if i could do something similar with textplot - run it on a big, chronologically-ordered corpus of documents, and see if it teases out a bird's-eye view of how the collection changes over time.

## Data preparation

As a test case, I decided to use the humanist list, the venerable digital humanisties email listserv started by Willard McCarty at the university of toronto in 1987. This seemed like a good place to start for a couple of reasons - for one, the complete, fulltext archives of the listserv can be downloaded directly from dhhumanist.org. And, email is inherently chronological, and in a really granular sense - the archive volumes take the form of big, 70-80mb text files, which just contain the raw feed of emails for each of the 27 years dating back to the beginning of 1987. Last, I was actually interested to use a corpus that I don't really know very much about - I've subscribed to the humanist for the last couple years, but I've read it spottily, and certainly don't know anything about what the listserv was like for the 25 years before 2012. In a way, though, this is actually an interesting opportunity to  test the usefulness of this kind of approach - without  reading the entire thing, what could I learn about it?

I downloaded all 27 of the year-long archive files, and started by writing a little python script to scrub out the large quantity of non-human-readable "header" information that gets prepended to most of the emails. Then, I concatenated all of the files together into as single `humanist.txt` file, which weighs in at a hefty 80mb. textplot parses this out into a cool 11.5 million words, consisting of 138,476 unique types. After making a couple of tweaks to the logic that  decides with words should be included in the network (I wanted to pick out the words that are most characteristic of on particular period in the history of the corpus, to get the best portrait of the diachronic shift - more on this later) and tinkering around with bandwidths and word counts, the network opened up into a big, spindly line - 1987 on the left, 2014 on the right:

[fig]

Almost immediately, I found myself tabbing back and forth between the browser and the iPython terminal, plugging in different terms to see how the density profiles of the words over time do (or don't) map onto the positions of the nodes in the network. This got annoying pretty quickly, so I decided to write some code that would take the raw GML (with the node layout information added by Gephi) and turn it into an interactive, d3-powered viewer that makes it easier to map the positions of the nodes in the network to the date range of the original corpus:

[fig]

As the network is panned and zoomed, the time axis at the bottom of the screen will automatically scale and re-center so that it always shows the (approximate) temporal range of the current viewport:

[fig]

And, to see the density function of an individual word, just click on the node label, which opens up a little chart that shows the kernel density estimation for that word, aligned with the "minimap" at the top right, which makes it easy to see how the final placement of the word in the network maps onto the density profile:

[fig]

The correspondence is pretty tight, thought not exact - the precise positions of the nodes are more meaningful for some words than for others. Generally, words at the edges of the network - both to the left and the right, but also the words that drift down below the middle - are the most temporally "focused," in the sense that they tend to occur in just one particular part of the corpus. This means that they often have really strong connections with other words that share a similar profile, which has the effect of forming little clumps of tightly-bound terms that drift away from the main body of words when passed through the force-directed layout. For example, "mainframe" spikes up really high in the winter of 1987, almost exactly in line with its position in the network, and then falls off pretty quickly over the course of the next five years:

[fig]

By contrast, many of the words closer to the center of the network are distributed much more evenly. For example, take a look at "technology," which has one of the highest pageranks of any of the terms in the corpus:

[fig]

It peaks out in 2002, pretty close to where it appears in the network, and there's definitely a leftward tilt in the density function - but it appears everywhere, and it's definitely less precisely "typical" or "characteristic" of 2004 than "mainframe" is of 1987.

## "Middle distance" reading the Humanist

The network turns into a kind of mini visual intellectual history of the list. To the left, back in 1987 and 88, the list is dominated words related to the hardware and software of the mid-80s - mainframe, microcomputer, workstation, printer, wordperfect, macintosh, vax, diskette, hypercard, bitnet, pc, compatible, command, modem, compuserve, telnet. It seems pragmatic and technical, grounded in the day-to-day of concerns of humanities computing in the late 80s. Beyond the technical terms, the academic conversation seems to center on language, literature, and textual studies - cyrillic, sanskrit, diacritics, arabic - and, interestingly and surely not unrelated, religious studies - bible, hebrew, theology, religious, church.

Then, over the course of the next decade, the semantic field gradually shifts in the direction of a set of terms that start to become much more clearly associated with the DH of the present moment - it's the progression of a field as it moves from the edges of the academy to the center and starts to organize itself as a structured discipline that exists in different places. There's a sudden explosion of place names in the mid 90s - philadelphia, pennsylvania, georgetown, quebec, rutgers, ottawa, colorado, lancashire, buffalo. In the first couple years of the aughts, there seems to be a broadening of scope, a turn outwards - press, national, forum, education, consortium, worldwide and issues, which dominates in 2002 and 2003, an interestingly flexible and polysemic word with notes both of intellectual engagement and of problems, difficulties, anxieties.

Around the same time, there's the beginning of a kind of meta-institutional awareness, an uptick in words that seem broadly related to the day-to-day work of administering an academic discipline - creating formal structures for exchanging and communicating ideas, preserving results, evaluating the quality of work, etc - dissemination, evaluation, preservation, speakers, invited, lecturer, dialogue, workshop, organised, proposals, interviews, submission, deadline. By the end of the decade, a distinctly type-2 DH is anchored by collaborative, and other words that orbit around the notion of DH as something that happens across traditional disciplinary and professional lines - team, intersection, technologists, disciplinary, interdisciplinary, alliance, roundtable.

The spatial turn happens around the same time (spatial, gis), and then the network drifts more or less into the current moment. phd and studentship, both of which have been gradually gaining ground since about the turn of the century, both surge around 2012, perhaps tracking the formalization of DH as a discrete field of study, instead of just a methodology that gets mixed in to existing disciplines? At the far right is a cluster of terms related to social media and modern web technologies, which provides a tidy counterweight to the 80's-era technologies to the left - gmail, wordpress, ipad, blogspot. And, of course, twitter and facebook, both of which peak out in unison before rapidly falling off in the spring of 2012, which seems to have been the moment of peak-social-media. It's also interesting to see just how relatively recent digitalhumanities and dh are, which only start to creep into the corpus in 2005.

On a personal level, this is fun for me because the history of the list also maps almost exactly onto my own life. I was born on June 25, 1987, just 44 days after Willard McCarty send the first message on May 12:

"This is test number 1. Please acknowledge."